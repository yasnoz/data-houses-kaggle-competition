{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this cell to regroup all your imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy import stats\n",
    "from tempfile import mkdtemp\n",
    "from shutil import rmtree\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from sklearn import set_config\n",
    "set_config(display = 'diagram')\n",
    "\n",
    "# Sklearn preprocessing\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import make_column_transformer, make_column_selector\n",
    "from sklearn.ensemble import AdaBoostRegressor, VotingRegressor, GradientBoostingRegressor, StackingRegressor, RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectPercentile, mutual_info_regression, VarianceThreshold, SelectFromModel\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.metrics import make_scorer, mean_squared_error, mean_squared_log_error\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üèÜ Le Wagon Kaggle Batch Challenge\n",
    "\n",
    "**Welcome to your first Kaggle competition!**\n",
    "\n",
    "<img src='https://wagon-public-datasets.s3.amazonaws.com/data-science-images/ML/kaggle-batch-challenge.png' width=600>\n",
    "\n",
    "Your objective is to **submit an answer (online)** to the open competition [House Prices - Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data) üè†\n",
    "\n",
    "Fortunately, you have already come across the housing dataset earlier in the bootcamp! You will be semi-guided toward a **baseline model**, and only after creating a baseline will you be free to improve and refine it. We will approach the problem using **pipelines** (the best practice)!\n",
    "\n",
    "A few words on Kaggle:\n",
    "- Kaggle will rank your submission amongst all participants!\n",
    "- Everyone is removed from the public leaderboard after 2 months\n",
    "- You can make up to 10 submissions per day\n",
    "\n",
    "üßπ Today is the perfect day to practice keeping your long notebook **tidy** üßπ\n",
    "- Collapse all headings from the command palette (`Cmd + Shift + P`)\n",
    "- Stay  \"idempotent\" (`Restart & Run All` should never crash)\n",
    "- Name and delete variables carefully"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle Setup\n",
    "\n",
    "üëâ Create an account on Kaggle if you want to participate in the competition\n",
    "\n",
    "üëâ Join the [House Prices Challenge](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data) \n",
    "\n",
    "üëâ Write down your Kaggle `username` in the [results spreadsheet here](https://docs.google.com/spreadsheets/d/1ZEBKwa_k1Ytb0WCOh-Nopq3eaezwBNu1SAqKXEXRguc/edit#gid=0); if you can't find your batch, reach out to your teacher!\n",
    "\n",
    "**The whole batch will compete as a group against the team of TAs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "In the challenge instructions, you should have already executed the steps to download everything you need from Kaggle into your current notebook folder:\n",
    "\n",
    "- `train.csv` is your `(1460, 81)` training set containing `X` and `y`\n",
    "- `test.csv` is your `(1459, 80)` testing set without the associated target `y` üòà\n",
    "- `sample_submission.csv` describes the format required to submit your answer\n",
    "\n",
    "‚ÑπÔ∏è You'll find a detailed description of the dataset [here](https://wagon-public-datasets.s3.amazonaws.com/05-Machine-Learning/07-Ensemble-Methods/kaggle_houses_data_description.txt). Refer to it throughout the challenge!\n",
    "\n",
    "Your goal is to predict the `y_pred` missing from your test set and submit it to discover your `test_score` and ranking\n",
    "\n",
    "‚ùì Load the training dataset into a DataFrame called `data`, and create your `X` and `y`. Inspect their shapes.\n",
    "\n",
    "**Hint:** if you check the CSV file, you will notice a column called `Id`. When reading the CSV file into a DF, make sure to set `index_col=\"Id\"` so that you don't get two ID columns üòâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üê£ 1. BASELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Initial feature overview\n",
    "\n",
    "79 features are too much to deal with one by one for a first baseline pipeline! Let's treat them solely based on their `dtype`:\n",
    "\n",
    "‚ùì How many numerical features vs. categorical features do we have? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Create a Series called `feat_categorical_nunique` containing the number of **unique values** for each categorical feature in our training set. How many unique categories are there in total?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§î If we were to `OneHotEncode` all categorical features, our feature matrix `X_preproc` would become pretty big and sparse, with almost 300 (highly correlated) features for only 1400 observations. Ideally, we should aim at feeding our model with a maximum of ~50 features (üìö read this [rule of thumb](https://datascience.stackexchange.com/a/11480/98300))\n",
    "\n",
    "We know 2 main strategies to reduce the number of categorical features post-preprocessing:\n",
    "1. **[Remove](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection)** features that bring too little explanation to our model; this may require statistical analysis of feature importance\n",
    "2. **[Ordinally encode](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html)** (instead of one-hot encode) categorical features into integers; this, however, creates a notion of \"order\" (1 > 2 > 3 > ...) that can be detrimental if not handled properly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Plot the **histogram** of the number of unique values per categorical feature. Do you see some quick wins?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° As a starting point, what about simply **removing** all features that have **7 unique values or more**, and one-hot encoding the rest? Let's keep ordinal encoding and statistical feature selection for the next iteration of our pipeline.\n",
    "\n",
    "‚ùì Store the names of the features to be OHE'd in a list called `feat_categorical_small` below. How many features will be OHE'd?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß™ Test your code below (and clear the cell once it passed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult(\n",
    "    'features_overview',\n",
    "    n=len(feat_categorical_small)\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Baseline Pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Preprocessing\n",
    "\n",
    "‚ùì Let's code the basic preprocessing pipeline described below. Save it under `preproc_baseline`.\n",
    "\n",
    "For categorical features:\n",
    "- Simple-Impute with the most frequent values\n",
    "- One-Hot Encode features that have less than 7 unique values to start with\n",
    "- Drop all other features\n",
    "\n",
    "\n",
    "As for numerical features:\n",
    "- Simple-Impute with strategy `mean`\n",
    "- Min-Max Scale\n",
    "\n",
    "\n",
    "<details>\n",
    "    <summary>‚ÑπÔ∏è Click here for a pro tip</summary>\n",
    "\n",
    "If you are confident, you can try Sklearn's shorter-syntax `make_pipeline` or `make_column_transformer` instead of the longer syntax of `Pipeline` or `ColumnTransformer`; also useful if you want to avoid giving names manually to every step.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Look at the **shape** of your preprocessed DataFrame and save it to `shape_preproc_baseline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß™ Test your code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult(\n",
    "    'preproc_baseline',\n",
    "    shape=shape_preproc_baseline\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Add Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Add a simple Decision Tree model to your `preproc_baseline` and store it to `pipe_baseline` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Cross-Validate\n",
    "\n",
    "‚ùì Read the Kaggle [contest evaluation rules](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview/evaluation). Which performance metric do you need? Is it readily available in Sklearn?\n",
    "\n",
    "Sadly, it isn't! We will need to create our custom `sklearn.metrics.scorer` object to pass to any cross-validation or Grid Search. The process is described below:\n",
    "\n",
    "\n",
    "1. Create a scorer called `rmsle` using [`make_scorer`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html) that can be passed as a value for the `scoring` `kwarg` like so:  \n",
    "    ```python\n",
    "    cross_val_score(pipe_baseline, X, y, cv=5, scoring=rmsle)\n",
    "    ```\n",
    "2.  Create its negative counterpart, `rmsle_neg`, which is best when _maximized_; this will come in handy later as `GridSearchCV` always tries to _maximize_ a score üòâ\n",
    "    ```python\n",
    "    GridSearchCV(pipe_baseline, param_grid=..., cv=5, scoring=rmsle_neg)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSLE formula\n",
    "\n",
    "$$\\text{RMSLE}(y, \\hat{y}) = \\sqrt{\\frac{1}{n_\\text{samples}} \\sum_{i=0}^{n_\\text{samples} - 1} (\\log_e (1 + y_i) - \\log_e (1 + \\hat{y}_i) )^2.}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì5-fold cross-validate your `pipe_baseline` using this metric to get a first glance at your baseline performance.    \n",
    "\n",
    "Store your mean score as `score_baseline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Predict Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Predict `y_pred_baseline` from the Kaggle `test.csv` dataset you stored in the `data` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Finally, store your ready-to-submit CSV as `submission_baseline.csv` in the `data` folder. **Carefully read** and understand Kaggle's required format and test it below (you don't need to submit this baseline to Kaggle for now)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß™ Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "tmp = pd.read_csv(\"data/submission_baseline.csv\")\n",
    "\n",
    "result = ChallengeResult(\n",
    "    'submission_baseline',\n",
    "    score_baseline = score_baseline,\n",
    "    submission_shape = tmp.shape,\n",
    "    submission_columns = list(tmp.columns),\n",
    "    submission_dtypes = str(list(tmp.dtypes)),\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üèãÔ∏è‚Äç‚ôÄÔ∏è 2. ITERATIONS \n",
    "\n",
    "üéâ üéâ Congratulation on having fully pipelined a baseline model! You will see now how much easier it is to iterate and improve performance üöÄ\n",
    "\n",
    "Now, your goal is to improve your prediction and submit it to Kaggle **at least 30 minutes before the Recap ‚è≥**\n",
    "\n",
    "We have some suggestions for improvements below: **pick your battles** and **incrementally** improve your pipeline as you see fit!\n",
    "\n",
    "**Estimators**\n",
    "\n",
    "- Tree-based ensembles (a must-try today); probably the best suited for problems with many categorical features\n",
    "- Stacking!\n",
    "- XGBoost!\n",
    "\n",
    "**Preprocessing** (once your first ensemble model works)\n",
    "\n",
    "- **Ordinal Encoding** of categorical features with a hidden notion of order in their values (e.g. \"bad\", \"average\", good\")\n",
    "- **Statistical Feature Selection** to remove useless features (avoids overfitting and reduces training time)\n",
    "- Predict `log(SalePrice)` instead?\n",
    "- ü§∑"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Preprocessing Iteration ‚ô≤ \n",
    "**‚ö†Ô∏è Come back here only after you have iterated on your estimators in section 2.2 ‚ö†Ô∏è**\n",
    "\n",
    "‚è© Collapse me if I'm not in use!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Ordinal Encoding (~1h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Look at the following feature. Couldn't it be encoded numerically in a wise manner?\n",
    "```\n",
    "ExterQual: Evaluates the quality of the material on the exterior \n",
    "\t\t\n",
    "       Ex\tExcellent\n",
    "       Gd\tGood\n",
    "       TA\tAverage/Typical\n",
    "       Fa\tFair\n",
    "       Po\tPoor\n",
    "```\n",
    "\n",
    "üí° Luckily, the `OrdinalEncoder` and its argument `categories`  allows us to do just that! Check it out below and make sure to understand how this works üëá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define specific order for features\n",
    "# Note: if you change this order, it will change the output for .transform()\n",
    "feature_A_sorted_values = ['bad', 'average', 'good']\n",
    "feature_B_sorted_values = ['dirty', 'clean', 'new']\n",
    "\n",
    "encoder = OrdinalEncoder(\n",
    "    categories=[\n",
    "        feature_A_sorted_values,\n",
    "        feature_B_sorted_values\n",
    "    ],\n",
    "    handle_unknown=\"use_encoded_value\",\n",
    "    unknown_value=-1\n",
    ")\n",
    "\n",
    "# Just some random training data\n",
    "XX = [\n",
    "    ['good', 'dirty'],\n",
    "    ['bad', 'new'],\n",
    "    ['average', 'clean'],\n",
    "]\n",
    "\n",
    "encoder.fit(XX)\n",
    "\n",
    "encoder.transform([\n",
    "        ['bad', \"dirty\"],\n",
    "        [\"average\", \"clean\"],\n",
    "        ['good', 'new'],\n",
    "        ['bad', 'oops never seen this label before']\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Your turn**: split your categorical preprocessor into\n",
    "\n",
    "- `preproc_ordinal` to ordinally encode **some features** (of your choice)\n",
    "- `preproc_nominal` to one-hot encode the other ones\n",
    "\n",
    "\n",
    "<details>\n",
    "    <summary>Hints</summary>\n",
    "\n",
    "- You won't be able to avoid hard-coding names and ordered values of features! Be tidy!\n",
    "- It's a good practice to sort your features alphabetically to avoid bad surprises\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Statistical Feature Selection (~30min)\n",
    "\n",
    "Our goal is to remove the least interesting features to limit overfitting and shorten training time.  \n",
    "\n",
    "üî• We will make use of Sklearn's [feature selection](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection) transformers directly in your pipeline!\n",
    "\n",
    "‚ùóÔ∏è We recommend you try **only Option 1 today**, to start with. Options 2 and 3 will be corrected in the Recap!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1 (Recommended) - <font color=green>Univariate</font> Feature Selection\n",
    "*based on their mutual information with target `y`*\n",
    "\n",
    "- Feel free to add a `SelectPercentile` filter at the end of your `preproc` pipeline.\n",
    "- This will filter out features that, taken individually, least explain your target!\n",
    "- The statistical test we recommend passing to SelectPercentile is the `mutual_info_regression`\n",
    "\n",
    "<details>\n",
    "    <summary markdown='span'>ü§î What is mutual information? Click here!</summary>\n",
    "\n",
    "- [Mutual Information](https://en.wikipedia.org/wiki/Mutual_information) is a **statistical** distance between two probability distributions\n",
    "- Correlation is a **linear** distance between two random variables\n",
    "- Mutual Information is more general and measures the reduction of uncertainty in Y after observing X.\n",
    "- On the other hand, if you already know you are working with variables that are smooth (like continuous numerical variables), sometimes correlation may tell you more about them, for instance if their relationship is monotonic.\n",
    "\n",
    "See [this animation](https://twitter.com/ari_seff/status/1409296508634152964)\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2 - <font color=green>Multivariate</font> Feature Selection\n",
    "*based on their combined relationship with target `y`*\n",
    "\n",
    "ü§î We want to remove features that do not help predict our target even when combined with all the others.\n",
    "\n",
    "1Ô∏è‚É£ To do so, remember that we can use the [`permutation_importance`](https://scikit-learn.org/stable/modules/permutation_importance.html) metric in combination with an estimator! It trains one pipe per feature to estimate which feature makes our performance score *decrease* the most when shuffling it randomly. These would be our most important features, which we don't want to remove.\n",
    "\n",
    "The best thing is that `scikit-learn` allows you to integrate this methodology directly into your `preproc` pipeline thanks to the [`SequentialFeatureSelector`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SequentialFeatureSelector.html) transformer; this will recursively remove the least important features according to the `cross_val_score`.\n",
    "\n",
    "When you have many features, however, this process can take extremely long to train.\n",
    "\n",
    "2Ô∏è‚É£ Alternatively, a faster way would be to make use of models that already output some measure of `feature_importance` when being fitted. For instance, trees with a Gini-based `feature_importance_`, or Lasso regressions with an L1 `coef_`. `scikit-learn` already has the [`SelectFromModel`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html) transformer to do just that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 3 - <font color=green>Unsupervised</font> Selection?\n",
    "*filter based only on the properties of `X`*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì A quick win is to remove features with the lowest variance. Think about it: a feature that only has one value is useless (and has a variance of 0).\n",
    "\n",
    "Feel free to add a [`VarianceThreshold`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html) to the end of your pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Additionally, we can check for correlation between our **numerical features** only\n",
    "\n",
    "- Use [Pearson's correlation](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) combined with a heatmap to visually check whether any **numerical** features almost entirely correlate with others\n",
    "- Use `VIF` from `statsmodels` to check for features that have the highest multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì For **ordinal features**, we can use [Spearman's rank correlation](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient) instead to check whether some **ordinally encoded** features are almost entirely \"ordered\" similarly to others. Feel free to plot a heatmap again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Now, feel free to create a \"filter\" in your pipeline that removes any feature you want beyond a given (Spearman + Pearson) correlation threshold; you'll need a custom transformer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Treat Cyclical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì We have some time-based features, why not **transform them** into cyclical features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Target Engineering (~15min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì We are asked to minimize the RMS**L**E. Why don't we transform our target to directly predict its `log`?\n",
    "- Check out the histogram of the target `y`\n",
    "- Normally distributed variables should be easier to predict with linear or parametric models\n",
    "- Create `y_log` and your new performance metrics\n",
    "- Don't forget to take the exponent of your predictions at the end!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Model Iteration ‚ôª"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Final Version of the Preproc Pipeline\n",
    "‚ùìWe advise you to start with a fresh definition below so you can quickly update it as needed and then try many model types to find the best one possible (you can try GridSearch or go model by model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üèÖFINAL SUBMISSION (submit at least 30 min before Recap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discover your real test score by submitting to Kaggle! \n",
    "\n",
    "üëâ Write down your test score on the [result spreadsheet here](https://docs.google.com/spreadsheets/d/1ZEBKwa_k1Ytb0WCOh-Nopq3eaezwBNu1SAqKXEXRguc/edit#gid=0) (pick the correct batch!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding columns to X_test according to what we did to X\n",
    "X_test['sin_MoSold'] = np.sin(2 * np.pi * (X_test.MoSold - 1) / months_in_a_year)\n",
    "X_test['cos_MoSold'] = np.cos(2 * np.pi * (X_test.MoSold - 1) / months_in_a_year)\n",
    "\n",
    "X_test.drop(columns=['MoSold'], inplace=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
